{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3464addf",
   "metadata": {},
   "source": [
    "## üß© Part 1 ‚Äî LangChain Core Concepts (Lecture 4.1)\n",
    "\n",
    "**Goal:** Get familiar with the LangChain components and connect them to real data.\n",
    "\n",
    "### Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf68df",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start Guide\n",
    "\n",
    "**To run this notebook successfully:**\n",
    "\n",
    "1. **Run Cell 2** - Install basic packages\n",
    "2. **Run Cell 4** - Install additional LangChain packages  \n",
    "3. **Create a `.env` file** in your workspace root with: `OPENROUTER_API_KEY=your_key_here`\n",
    "4. **Restart the kernel** (to clear any import cache)\n",
    "5. **Run Cell 6** - Import libraries (should work without errors now!)\n",
    "6. **Continue running cells sequentially**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58aebb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages one by one for clarity\n",
    "%pip install langchain\n",
    "%pip install langchain-community\n",
    "%pip install langchain-openai\n",
    "%pip install openai\n",
    "%pip install faiss-cpu\n",
    "%pip install tiktoken\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e70aa",
   "metadata": {},
   "source": [
    "**üí° Important Note:** The next cell installs the **FULL `langchain` package** which includes the `chains` module. This is essential for `RetrievalQA` to work. The modular packages alone (`langchain-core`, `langchain-community`) don't include chains functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the FULL langchain package (includes chains module)\n",
    "# This is the KEY package that was missing!\n",
    "%pip install --upgrade langchain\n",
    "\n",
    "# Also install these supporting packages\n",
    "%pip install langchain-core\n",
    "%pip install langchain-text-splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812b1e79",
   "metadata": {},
   "source": [
    "### Step 2: Import Core Modules and Configure OpenRouter API\n",
    "\n",
    "**Important:** Make sure you have your OpenRouter API key set in a `.env` file:\n",
    "```\n",
    "OPENROUTER_API_KEY=your_key_here\n",
    "```\n",
    "\n",
    "Get your free API key from [https://openrouter.ai/keys](https://openrouter.ai/keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98edfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported and OpenRouter configured successfully!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OPENROUTER_API_KEY not found! Please set it in your .env file\")\n",
    "\n",
    "# Configure OpenRouter for both embeddings and chat\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
    "\n",
    "print(\"‚úÖ Libraries imported and OpenRouter configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85770e20",
   "metadata": {},
   "source": [
    "**üîß Note on LangChain Version:** This notebook uses **LangChain 1.2.0+**, which has a different API than older versions. The `RetrievalQA` class has been replaced with newer chain construction methods. We've created helper functions to maintain the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57d31ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions created!\n"
     ]
    }
   ],
   "source": [
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Helper function to create a QA chain\n",
    "def create_qa_chain(llm, retriever):\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"Answer the question based only on the following context:\\n\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    # Wrapper to return source documents\n",
    "    def qa_with_sources(query):\n",
    "        docs = retriever.invoke(query)\n",
    "        answer = rag_chain.invoke(query)\n",
    "        return {\"result\": answer, \"source_documents\": docs}\n",
    "    \n",
    "    return qa_with_sources\n",
    "\n",
    "print(\"‚úÖ Helper functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63785ebf",
   "metadata": {},
   "source": [
    "### Step 3: Create Sample Documents\n",
    "\n",
    "These documents will form the knowledge base for our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f06b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 6 documents\n",
      "1. Retrieval-Augmented Generation (RAG) enhances LLM accuracy b...\n",
      "2. LangChain provides modular tools to connect LLMs with extern...\n",
      "3. FAISS is used for fast vector search and similarity-based re...\n",
      "4. OpenRouter provides unified access to multiple LLM providers...\n",
      "5. Vector databases store embeddings and enable semantic search...\n",
      "6. Text chunking is essential for managing context windows in L...\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Retrieval-Augmented Generation (RAG) enhances LLM accuracy by grounding responses in real data.\",\n",
    "    \"LangChain provides modular tools to connect LLMs with external knowledge sources.\",\n",
    "    \"FAISS is used for fast vector search and similarity-based retrieval.\",\n",
    "    \"OpenRouter provides unified access to multiple LLM providers through a single API.\",\n",
    "    \"Vector databases store embeddings and enable semantic search capabilities.\",\n",
    "    \"Text chunking is essential for managing context windows in LLMs.\"\n",
    "]\n",
    "\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "print(f\"‚úÖ Created {len(docs)} documents\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"{i+1}. {doc.page_content[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19f8706",
   "metadata": {},
   "source": [
    "### Step 4: Chunk Data for Embedding\n",
    "\n",
    "Splitting text into smaller chunks ensures better retrieval accuracy and manages context window limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a561f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 6\n",
      "\n",
      "First 3 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Content: Retrieval-Augmented Generation (RAG) enhances LLM accuracy by grounding responses in real data.\n",
      "Length: 95 characters\n",
      "\n",
      "Chunk 2:\n",
      "Content: LangChain provides modular tools to connect LLMs with external knowledge sources.\n",
      "Length: 81 characters\n",
      "\n",
      "Chunk 3:\n",
      "Content: FAISS is used for fast vector search and similarity-based retrieval.\n",
      "Length: 68 characters\n"
     ]
    }
   ],
   "source": [
    "splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=10)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "print(f\"‚úÖ Total chunks created: {len(chunks)}\")\n",
    "print(\"\\nFirst 3 chunks:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"Content: {chunk.page_content}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf17c65",
   "metadata": {},
   "source": [
    "**‚úÖ Output:** You should see multiple small text chunks ready for vectorization ‚Äî these are the atomic units RAG retrieves later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c3c36",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Part 2 ‚Äî RAG Implementation with LangChain (Lecture 4.2)\n",
    "\n",
    "**Goal:** Build a functional RAG pipeline: Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate.\n",
    "\n",
    "### Step 1: Create Embeddings and Store in FAISS\n",
    "\n",
    "We'll use OpenRouter's embedding endpoint (OpenAI-compatible) to generate vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183aa1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating embeddings and building FAISS index...\n",
      "‚úÖ Vectorstore created and retriever configured!\n",
      "üìä Index contains 6 vectors\n",
      "<langchain_community.vectorstores.faiss.FAISS object at 0x0000026B27A937D0>\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings using OpenRouter\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"openai/text-embedding-3-small\",\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    openai_api_key=api_key\n",
    ")\n",
    "\n",
    "# Create FAISS vectorstore from documents\n",
    "print(\"üîÑ Creating embeddings and building FAISS index...\")\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Create retriever with similarity search\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\", \n",
    "    search_kwargs={\"k\": 2}  # Retrieve top 2 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Vectorstore created and retriever configured!\")\n",
    "print(f\"üìä Index contains {vectorstore.index.ntotal} vectors\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c140c",
   "metadata": {},
   "source": [
    "### Step 2: Initialize LLM\n",
    "\n",
    "We'll use OpenRouter to access various LLM models. Here we're using a fast and capable model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b16c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM initialized successfully!\n",
      "üìã Model: meta-llama/llama-3.1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChatOpenAI with OpenRouter\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"meta-llama/llama-3.1-8b-instruct\",  # Fast, capable model\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.3  # Lower temperature for more focused responses\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized successfully!\")\n",
    "print(f\"üìã Model: meta-llama/llama-3.1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cb388b",
   "metadata": {},
   "source": [
    "### Step 3: Build the RAG Chain\n",
    "\n",
    "The RetrievalQA chain combines the retriever and LLM to create a question-answering system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87d859ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG chain created successfully!\n",
      "üéØ Ready to answer questions!\n"
     ]
    }
   ],
   "source": [
    "# Build the RAG chain\n",
    "qa_chain = create_qa_chain(llm, retriever)\n",
    "\n",
    "print(\"‚úÖ RAG chain created successfully!\")\n",
    "print(\"üéØ Ready to answer questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154d108",
   "metadata": {},
   "source": [
    "### Step 4: Ask Your RAG System a Question\n",
    "\n",
    "Let's test the pipeline with a query about LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766d53b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Query: What is the benefit of using LangChain in RAG?\n",
      "\n",
      "üîÑ Processing...\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "The benefit of using LangChain in RAG is that it enhances LLM accuracy by grounding responses in real data.\n",
      "\n",
      "======================================================================\n",
      "üìö CONTEXT USED:\n",
      "======================================================================\n",
      "\n",
      "Source 1:\n",
      "LangChain provides modular tools to connect LLMs with external knowledge sources.\n",
      "\n",
      "Source 2:\n",
      "Retrieval-Augmented Generation (RAG) enhances LLM accuracy by grounding responses in real data.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the benefit of using LangChain in RAG?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query}\\n\")\n",
    "print(\"üîÑ Processing...\")\n",
    "\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö CONTEXT USED:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result[\"source_documents\"]):\n",
    "    print(f\"\\nSource {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307ee51",
   "metadata": {},
   "source": [
    "**‚úÖ Expected Output:** A concise, context-grounded answer referencing LangChain's modular structure and role in retrieval workflows.\n",
    "\n",
    "### Step 5: Test with Another Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8439cf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Query: How does FAISS help in RAG systems?\n",
      "\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "FAISS helps in RAG systems by enabling fast vector search and similarity-based retrieval, which is crucial for efficiently retrieving relevant data to ground responses in real data.\n",
      "\n",
      "======================================================================\n",
      "üìö CONTEXT USED:\n",
      "======================================================================\n",
      "\n",
      "Source 1:\n",
      "FAISS is used for fast vector search and similarity-based retrieval.\n",
      "\n",
      "Source 2:\n",
      "Retrieval-Augmented Generation (RAG) enhances LLM accuracy by grounding responses in real data.\n"
     ]
    }
   ],
   "source": [
    "query2 = \"How does FAISS help in RAG systems?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query2}\\n\")\n",
    "result2 = qa_chain(query2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üí° ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result2[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö CONTEXT USED:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result2[\"source_documents\"]):\n",
    "    print(f\"\\nSource {i+1}:\")\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d069c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß† Part 3 ‚Äî Adding Context and Metadata (Lecture 4.3)\n",
    "\n",
    "**Goal:** Enhance the pipeline by attaching metadata such as document source, category, or author ‚Äî and show how it improves retrieval context.\n",
    "\n",
    "### Step 1: Rebuild Documents with Metadata\n",
    "\n",
    "Metadata enriches your knowledge base with provenance information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e07a9965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 6 documents with metadata\n",
      "\n",
      "Sample document with metadata:\n",
      "Content: LangChain helps connect LLMs with external knowledge bases.\n",
      "Metadata: {'source': 'LangChain Docs', 'category': 'Framework', 'date': '2024'}\n"
     ]
    }
   ],
   "source": [
    "# Create documents with rich metadata\n",
    "docs_with_metadata = [\n",
    "    Document(\n",
    "        page_content=\"LangChain helps connect LLMs with external knowledge bases.\",\n",
    "        metadata={\"source\": \"LangChain Docs\", \"category\": \"Framework\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG systems improve factual accuracy by combining retrieval and generation.\",\n",
    "        metadata={\"source\": \"OpenAI Blog\", \"category\": \"AI Research\", \"date\": \"2023\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"FAISS allows efficient vector similarity search for embeddings.\",\n",
    "        metadata={\"source\": \"Meta Research\", \"category\": \"Database\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"OpenRouter provides a unified API to access multiple AI models from different providers.\",\n",
    "        metadata={\"source\": \"OpenRouter Docs\", \"category\": \"Platform\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Embeddings transform text into dense vector representations that capture semantic meaning.\",\n",
    "        metadata={\"source\": \"AI Fundamentals\", \"category\": \"AI Research\", \"date\": \"2023\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Text chunking strategies significantly impact RAG system performance and accuracy.\",\n",
    "        metadata={\"source\": \"RAG Best Practices\", \"category\": \"Tutorial\", \"date\": \"2024\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(docs_with_metadata)} documents with metadata\\n\")\n",
    "print(\"Sample document with metadata:\")\n",
    "print(f\"Content: {docs_with_metadata[0].page_content}\")\n",
    "print(f\"Metadata: {docs_with_metadata[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801de82",
   "metadata": {},
   "source": [
    "### Step 2: Recreate the FAISS Vectorstore with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b24b924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating new vectorstore with metadata...\n",
      "‚úÖ Vectorstore and chain recreated with metadata!\n"
     ]
    }
   ],
   "source": [
    "# Recreate vectorstore with metadata-enriched documents\n",
    "print(\"üîÑ Creating new vectorstore with metadata...\")\n",
    "vectorstore_with_metadata = FAISS.from_documents(docs_with_metadata, embeddings)\n",
    "\n",
    "retriever_with_metadata = vectorstore_with_metadata.as_retriever(\n",
    "    search_kwargs={\"k\": 2}\n",
    ")\n",
    "\n",
    "# Rebuild the QA chain with the new retriever\n",
    "qa_chain_with_metadata = create_qa_chain(llm, retriever_with_metadata)\n",
    "\n",
    "print(\"‚úÖ Vectorstore and chain recreated with metadata!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2659a196",
   "metadata": {},
   "source": [
    "### Step 3: Query with Context and Display Metadata\n",
    "\n",
    "Now let's ask a question and see how metadata enriches the response context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "874d1cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Query: Which tool helps with efficient vector search?\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "FAISS\n",
      "\n",
      "======================================================================\n",
      "üìö METADATA FROM RETRIEVED DOCS:\n",
      "======================================================================\n",
      "\n",
      "üîπ Document 1:\n",
      "   Content: FAISS allows efficient vector similarity search for embeddings.\n",
      "   Source: Meta Research\n",
      "   Category: Database\n",
      "   Date: 2024\n",
      "\n",
      "üîπ Document 2:\n",
      "   Content: Embeddings transform text into dense vector representations that capture semantic meaning.\n",
      "   Source: AI Fundamentals\n",
      "   Category: AI Research\n",
      "   Date: 2023\n"
     ]
    }
   ],
   "source": [
    "query3 = \"Which tool helps with efficient vector search?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query3}\\n\")\n",
    "result3 = qa_chain_with_metadata(query3)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result3[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö METADATA FROM RETRIEVED DOCS:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result3[\"source_documents\"]):\n",
    "    print(f\"\\nüîπ Document {i+1}:\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Source: {doc.metadata['source']}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Date: {doc.metadata['date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d55991d",
   "metadata": {},
   "source": [
    "**‚úÖ Expected Output:** Answer mentions \"FAISS,\" with metadata showing it came from the \"Meta Research\" source.\n",
    "\n",
    "### Step 4: Advanced Query with Metadata Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d39b422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì Query: What are the key components of a RAG system?\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "Retrieval and Generation.\n",
      "\n",
      "======================================================================\n",
      "üìö SOURCES AND METADATA:\n",
      "======================================================================\n",
      "\n",
      "üîπ Source 1: OpenAI Blog (AI Research)\n",
      "   Content: RAG systems improve factual accuracy by combining retrieval and generation.\n",
      "   Published: 2023\n",
      "\n",
      "üîπ Source 2: RAG Best Practices (Tutorial)\n",
      "   Content: Text chunking strategies significantly impact RAG system performance and accuracy.\n",
      "   Published: 2024\n"
     ]
    }
   ],
   "source": [
    "query4 = \"What are the key components of a RAG system?\"\n",
    "\n",
    "print(f\"‚ùì Query: {query4}\\n\")\n",
    "result4 = qa_chain_with_metadata(query4)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result4[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö SOURCES AND METADATA:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result4[\"source_documents\"]):\n",
    "    print(f\"\\nüîπ Source {i+1}: {doc.metadata['source']} ({doc.metadata['category']})\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Published: {doc.metadata['date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2a6086",
   "metadata": {},
   "source": [
    "### Step 5: Test Retrieval by Metadata Category\n",
    "\n",
    "Let's see what documents we have in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f1fe86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Documents by Category:\n",
      "\n",
      "üè∑Ô∏è  Framework:\n",
      "   ‚Ä¢ LangChain helps connect LLMs with external knowled...\n",
      "\n",
      "üè∑Ô∏è  AI Research:\n",
      "   ‚Ä¢ RAG systems improve factual accuracy by combining ...\n",
      "   ‚Ä¢ Embeddings transform text into dense vector repres...\n",
      "\n",
      "üè∑Ô∏è  Database:\n",
      "   ‚Ä¢ FAISS allows efficient vector similarity search fo...\n",
      "\n",
      "üè∑Ô∏è  Platform:\n",
      "   ‚Ä¢ OpenRouter provides a unified API to access multip...\n",
      "\n",
      "üè∑Ô∏è  Tutorial:\n",
      "   ‚Ä¢ Text chunking strategies significantly impact RAG ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analyze metadata distribution\n",
    "from collections import defaultdict\n",
    "\n",
    "category_docs = defaultdict(list)\n",
    "for doc in docs_with_metadata:\n",
    "    category_docs[doc.metadata['category']].append(doc.page_content[:50] + \"...\")\n",
    "\n",
    "print(\"üìä Documents by Category:\\n\")\n",
    "for category, contents in category_docs.items():\n",
    "    print(f\"üè∑Ô∏è  {category}:\")\n",
    "    for content in contents:\n",
    "        print(f\"   ‚Ä¢ {content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c2165",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üß≠ Bonus Challenges\n",
    "\n",
    "Try these extensions to deepen your understanding:\n",
    "\n",
    "### Challenge 1: Add New Document Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f901dcc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Adding new 'Use Cases' category documents...\n",
      "‚úÖ Extended vectorstore now contains 9 documents!\n",
      "\n",
      "‚ùì Query: What are some real-world applications of RAG systems?\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER:\n",
      "======================================================================\n",
      "Healthcare providers and e-commerce companies use RAG systems for diagnosis support and product recommendations, respectively.\n",
      "\n",
      "======================================================================\n",
      "üìö SOURCES (showing Use Cases):\n",
      "======================================================================\n",
      "\n",
      "üîπ Use Cases: Medical AI Journal\n",
      "   Healthcare providers leverage RAG systems to query medical knowledge bases for diagnosis support.\n",
      "\n",
      "üîπ AI Research: OpenAI Blog\n",
      "   RAG systems improve factual accuracy by combining retrieval and generation.\n",
      "\n",
      "üîπ Use Cases: Industry Report\n",
      "   E-commerce companies use RAG to provide accurate product recommendations based on inventory data.\n"
     ]
    }
   ],
   "source": [
    "# Add documents with \"Use Cases\" category\n",
    "new_docs = [\n",
    "    Document(\n",
    "        page_content=\"E-commerce companies use RAG to provide accurate product recommendations based on inventory data.\",\n",
    "        metadata={\"source\": \"Industry Report\", \"category\": \"Use Cases\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Healthcare providers leverage RAG systems to query medical knowledge bases for diagnosis support.\",\n",
    "        metadata={\"source\": \"Medical AI Journal\", \"category\": \"Use Cases\", \"date\": \"2024\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Legal firms employ RAG to search and analyze case law and legal precedents efficiently.\",\n",
    "        metadata={\"source\": \"Legal Tech Review\", \"category\": \"Use Cases\", \"date\": \"2024\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine with existing documents\n",
    "all_docs = docs_with_metadata + new_docs\n",
    "\n",
    "# Recreate vectorstore\n",
    "print(\"üîÑ Adding new 'Use Cases' category documents...\")\n",
    "vectorstore_extended = FAISS.from_documents(all_docs, embeddings)\n",
    "retriever_extended = vectorstore_extended.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "qa_chain_extended = create_qa_chain(llm, retriever_extended)\n",
    "\n",
    "print(f\"‚úÖ Extended vectorstore now contains {len(all_docs)} documents!\")\n",
    "\n",
    "# Test with use-case query\n",
    "query5 = \"What are some real-world applications of RAG systems?\"\n",
    "print(f\"\\n‚ùì Query: {query5}\\n\")\n",
    "result5 = qa_chain_extended(query5)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER:\")\n",
    "print(\"=\"*70)\n",
    "print(result5[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö SOURCES (showing Use Cases):\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result5[\"source_documents\"]):\n",
    "    print(f\"\\nüîπ {doc.metadata['category']}: {doc.metadata['source']}\")\n",
    "    print(f\"   {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38df55",
   "metadata": {},
   "source": [
    "### Challenge 2: Experiment with Different LLM Models\n",
    "\n",
    "OpenRouter gives you access to many models. Let's try a different one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da2b860c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing with alternative model: anthropic/claude-3-haiku\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER (from Claude Haiku):\n",
      "======================================================================\n",
      "Embeddings play a crucial role in enabling semantic search within RAG (Retrieval-Augmented Generation) systems. Here's how they contribute to this process:\n",
      "\n",
      "1. Transforming text into vector representations:\n",
      "   Embeddings transform the input text, whether it's a query or a passage from a knowledge base, into dense vector representations. These vector representations capture the semantic meaning and relationships between the words, phrases, or documents.\n",
      "\n",
      "2. Capturing semantic similarity:\n",
      "   The vector representations created by embeddings preserve the semantic similarity between different textual inputs. Words or passages that are semantically related will have vector representations that are closer to each other in the vector space, while unrelated inputs will have more distant vector representations.\n",
      "\n",
      "3. Enabling semantic search:\n",
      "   In a RAG system, the query provided by the user is first transformed into an embedding vector. This vector is then used to search the embeddings of the passages or documents in the knowledge base. The system can identify the most semantically relevant passages by finding the ones with the closest vector representations to the query embedding.\n",
      "\n",
      "4. Improving retrieval accuracy:\n",
      "   By leveraging the semantic information captured by embeddings, RAG systems can retrieve passages that are more relevant to the user's query, even if the exact wording does not match. This helps improve the factual accuracy of the system's responses, as it can identify the most appropriate information from the knowledge base to address the user's needs.\n",
      "\n",
      "In the context of healthcare providers using RAG systems for diagnosis support, embeddings enable these systems to understand the semantic meaning of the user's query and match it with the most relevant medical knowledge from the knowledge base. This allows the healthcare providers to receive more accurate and relevant information to assist in the diagnostic process.\n"
     ]
    }
   ],
   "source": [
    "# Try a different model from OpenRouter\n",
    "llm_alternative = ChatOpenAI(\n",
    "    model_name=\"anthropic/claude-3-haiku\",  # Alternative: fast Claude model\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "    openai_api_key=api_key,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "qa_chain_alternative = create_qa_chain(llm_alternative, retriever_extended)\n",
    "\n",
    "print(\"üîÑ Testing with alternative model: anthropic/claude-3-haiku\\n\")\n",
    "\n",
    "query6 = \"Explain how embeddings enable semantic search in RAG systems.\"\n",
    "result6 = qa_chain_alternative(query6)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER (from Claude Haiku):\")\n",
    "print(\"=\"*70)\n",
    "print(result6[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da44992",
   "metadata": {},
   "source": [
    "### Challenge 3: Implement MMR (Maximum Marginal Relevance) Retrieval\n",
    "\n",
    "MMR promotes diversity in retrieved results, reducing redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e9dabc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Testing MMR retrieval for diversity...\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER (with MMR retrieval):\n",
      "======================================================================\n",
      "AI technologies and tools mentioned include:\n",
      "\n",
      "* OpenRouter: a unified API for accessing multiple AI models from different providers\n",
      "* RAG (Relevance Aware Generator) system: a text generation system that uses text chunking strategies\n",
      "* FAISS: a library for efficient vector similarity search, particularly useful for embeddings.\n",
      "\n",
      "======================================================================\n",
      "üìö DIVERSE SOURCES RETRIEVED:\n",
      "======================================================================\n",
      "\n",
      "üîπ Platform: OpenRouter provides a unified API to access multiple AI models from different pr...\n",
      "\n",
      "üîπ Tutorial: Text chunking strategies significantly impact RAG system performance and accurac...\n",
      "\n",
      "üîπ Database: FAISS allows efficient vector similarity search for embeddings....\n"
     ]
    }
   ],
   "source": [
    "# Create retriever with MMR search type\n",
    "retriever_mmr = vectorstore_extended.as_retriever(\n",
    "    search_type=\"mmr\",  # Maximum Marginal Relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        \"fetch_k\": 10,  # Fetch 10 candidates, return 3 diverse ones\n",
    "        \"lambda_mult\": 0.5  # Balance between relevance (1.0) and diversity (0.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "qa_chain_mmr = create_qa_chain(llm, retriever_mmr)\n",
    "\n",
    "print(\"üéØ Testing MMR retrieval for diversity...\\n\")\n",
    "\n",
    "query7 = \"Tell me about AI technologies and tools.\"\n",
    "result7 = qa_chain_mmr(query7)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER (with MMR retrieval):\")\n",
    "print(\"=\"*70)\n",
    "print(result7[\"result\"])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìö DIVERSE SOURCES RETRIEVED:\")\n",
    "print(\"=\"*70)\n",
    "for i, doc in enumerate(result7[\"source_documents\"]):\n",
    "    print(f\"\\nüîπ {doc.metadata['category']}: {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df273b6",
   "metadata": {},
   "source": [
    "### Challenge 4: Custom Prompt Template\n",
    "\n",
    "Create a custom prompt to guide the LLM's response style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aca1e76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Testing with custom prompt template...\n",
      "\n",
      "======================================================================\n",
      "üí° ANSWER (with custom prompt):\n",
      "======================================================================\n",
      "FAISS (Facebook AI Similarity Search) is a library developed by Facebook AI Research (FAIR) that allows for efficient vector similarity search for embeddings. This makes it suitable for RAG (Retrieval-Augmented Generation) applications, such as searching and analyzing case law and legal precedents, as employed by legal firms.\n",
      "\n",
      "The efficiency of FAISS in RAG applications can be attributed to its ability to:\n",
      "\n",
      "1. **High-performance similarity search**: FAISS uses a combination of techniques, including quantization and indexing, to enable fast and efficient similarity search of dense vectors (embeddings) (Johnson et al., 2017). This is particularly useful in RAG applications where the system needs to quickly retrieve relevant documents or case law precedents from a large corpus.\n",
      "2. **Scalability**: FAISS is designed to handle large-scale datasets and can efficiently search through millions of vectors (Johnson et al., 2017). This scalability is essential in RAG applications where the corpus of case law and legal precedents can be vast and constantly growing.\n",
      "3. **Flexibility**: FAISS supports various indexing and search algorithms, allowing users to choose the most suitable approach for their specific use case (Johnson et al., 2017). This flexibility is beneficial in RAG applications where the system needs to adapt to different types of legal documents and case law.\n",
      "\n",
      "In the context of RAG applications, FAISS enables efficient retrieval of relevant documents or case law precedents, which can then be used to augment the generation of high-quality text. This improves the factual accuracy of the generated text by incorporating relevant information from the retrieved documents.\n",
      "\n",
      "References:\n",
      "Johnson, J., Douze, M., & J√©gou, H. (2017). Billion-scale similarity search with GPUs. arXiv preprint arXiv:1706.02282.\n"
     ]
    }
   ],
   "source": [
    "# Define custom prompt template\n",
    "custom_prompt = ChatPromptTemplate.from_template(\"\"\"You are a helpful AI assistant specializing in RAG systems and AI technologies.\n",
    "Use the following pieces of context to answer the question at the end. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Always cite which source(s) you used in your answer.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Detailed Answer:\"\"\")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "def create_custom_qa_chain(llm, retriever, prompt):\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    def qa_with_sources(query):\n",
    "        docs = retriever.invoke(query)\n",
    "        answer = rag_chain.invoke(query)\n",
    "        return {\"result\": answer, \"source_documents\": docs}\n",
    "    \n",
    "    return qa_with_sources\n",
    "\n",
    "qa_chain_custom = create_custom_qa_chain(llm, retriever_extended, custom_prompt)\n",
    "\n",
    "print(\"üé® Testing with custom prompt template...\\n\")\n",
    "\n",
    "query8 = \"What makes FAISS suitable for RAG applications?\"\n",
    "result8 = qa_chain_custom(query8)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"üí° ANSWER (with custom prompt):\")\n",
    "print(\"=\"*70)\n",
    "print(result8[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7c5d8f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üßæ Summary and Deliverables\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "‚úÖ **Part 1:** Built the foundation with LangChain components\n",
    "- Created and chunked documents\n",
    "- Prepared data for vectorization\n",
    "\n",
    "‚úÖ **Part 2:** Implemented a complete RAG pipeline\n",
    "- Generated embeddings using OpenRouter\n",
    "- Stored vectors in FAISS\n",
    "- Created a retrieval-augmented QA system\n",
    "\n",
    "‚úÖ **Part 3:** Enhanced with metadata and context\n",
    "- Added provenance information to documents\n",
    "- Demonstrated metadata-aware retrieval\n",
    "- Improved answer attribution and transparency\n",
    "\n",
    "‚úÖ **Bonus Challenges:** Extended the system with\n",
    "- Additional document categories\n",
    "- Alternative LLM models\n",
    "- MMR diversity-based retrieval\n",
    "- Custom prompt templates\n",
    "\n",
    "### Key Concepts Mastered:\n",
    "\n",
    "1. **LangChain Architecture:** Document loaders, text splitters, embeddings, vector stores, retrievers, and chains\n",
    "2. **RAG Pipeline:** Embed ‚Üí Store ‚Üí Retrieve ‚Üí Generate workflow\n",
    "3. **OpenRouter Integration:** Using multiple AI providers through a unified API\n",
    "4. **Metadata Management:** Enriching context with source attribution\n",
    "5. **Retrieval Strategies:** Similarity search vs. MMR for different use cases\n",
    "6. **Prompt Engineering:** Customizing LLM behavior with prompt templates\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- üîπ Explore different chunking strategies (semantic, recursive)\n",
    "- üîπ Try alternative vector databases (Chroma, Pinecone, Weaviate)\n",
    "- üîπ Implement re-ranking for improved retrieval quality\n",
    "- üîπ Add conversation memory for multi-turn interactions\n",
    "- üîπ Deploy your RAG system as a web application\n",
    "\n",
    "---\n",
    "\n",
    "## üì∏ Screenshot Checklist\n",
    "\n",
    "Capture these for your deliverables:\n",
    "1. ‚úÖ Successful embedding generation output\n",
    "2. ‚úÖ Question with answer and retrieved context\n",
    "3. ‚úÖ Metadata display showing source attribution\n",
    "4. ‚úÖ Comparison between different retrieval strategies\n",
    "5. ‚úÖ Custom prompt template results\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've built a production-ready RAG system from scratch using LangChain and OpenRouter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
